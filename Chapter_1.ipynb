{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6826190-6f8b-4d42-9c75-0dfacd07e677",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6f6ec-b777-4205-bb4d-02a244104e41",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a branch of Machine Learning which addresses the problem of automatic learning of optimal decisions over time\n",
    "\n",
    "Supervised learning: learn a function to map inputs into outputs given some \"resolved\" or labeled examples\n",
    "\n",
    "Unsupervised: find hidden patterns and relations in the data\n",
    "\n",
    "Reinforcement learning: reward system\n",
    "Reward: positive or negative feedback from the environment, guiding the agent to success. It is local, meaning that it reflects the success of the agent's recent activity, not all the success achieved so far. The agent tries to achieve the largest accumulated reward over its sequence of actions\n",
    "Agent: Interacts with the environment by executing actions, taking observations and receiving eventual rewards for this\n",
    "Environment: Everything outside the agent. It communicates with the agent via rewards (given by the environmet), actions (done on the environment) and some observations\n",
    "Actions: Things the agent is allowed to do in/to the environment\n",
    "Observations: What the agent can get from the environment. It's different from the environment state, which can very complicated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0226d-8971-41a6-bbce-370a48c21d8e",
   "metadata": {},
   "source": [
    "Reinforcement learning is based on Markov chains\n",
    "State space: all possible states for a system\n",
    "A sequence of observed states over time is called **history**\n",
    "For example, a simple model of the weather in some city may be composed of {sunny, cloudy}\n",
    "A chain of states [sunny, cloudy, ...] would be a history\n",
    "To this chain to be called a Markov Process, it needs to fulfil the Markov Property: the next state only depends on the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3382e9a-4761-44c2-87f9-d2cae99feb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
